# -*- coding: utf-8 -*-
"""Personal_Loan_Campaign.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rQUjTiDRRmgwBYN6C9g-0VZ8a2vEh8O0

## Problem Statement

### Context

AllLife Bank is a US bank that has a growing customer base. The majority of these customers are liability customers (depositors) with varying sizes of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).

A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio.

You as a Data scientist at AllLife bank have to build a model that will help the marketing department to identify the potential customers who have a higher probability of purchasing the loan.

### Objective

To predict whether a liability customer will buy personal loans, to understand which customer attributes are most significant in driving purchases, and identify which segment of customers to target more.

### Data Dictionary
* `ID`: Customer ID
* `Age`: Customer’s age in completed years
* `Experience`: #years of professional experience
* `Income`: Annual income of the customer (in thousand dollars)
* `ZIP Code`: Home Address ZIP code.
* `Family`: the Family size of the customer
* `CCAvg`: Average spending on credit cards per month (in thousand dollars)
* `Education`: Education Level. 1: Undergrad; 2: Graduate;3: Advanced/Professional
* `Mortgage`: Value of house mortgage if any. (in thousand dollars)
* `Personal_Loan`: Did this customer accept the personal loan offered in the last campaign? (0: No, 1: Yes)
* `Securities_Account`: Does the customer have securities account with the bank? (0: No, 1: Yes)
* `CD_Account`: Does the customer have a certificate of deposit (CD) account with the bank? (0: No, 1: Yes)
* `Online`: Do customers use internet banking facilities? (0: No, 1: Yes)
* `CreditCard`: Does the customer use a credit card issued by any other Bank (excluding All life Bank)? (0: No, 1: Yes)

## Importing necessary libraries
"""

# Installing the libraries with the specified version.
!pip install numpy==1.25.2 pandas==1.5.3 matplotlib==3.7.1 seaborn==0.13.1 scikit-learn==1.2.2 sklearn-pandas==2.2.0 -q --user

"""**Note**:

1. After running the above cell, kindly restart the notebook kernel (for Jupyter Notebook) or runtime (for Google Colab), write the relevant code for the project from the next cell, and run all cells sequentially from the next cell.

2. On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook.
"""

# Libraries to help with reading and manipulating data
import pandas as pd
import numpy as np

# libaries to help with data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Library to split data
from sklearn.model_selection import train_test_split

# To build model for prediction
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# To get diferent metric scores
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
)

# to suppress unnecessary warnings
import warnings
warnings.filterwarnings("ignore")

"""## Loading the dataset"""

from google.colab import drive
drive.mount('/content/drive')

#read the dataset
data = pd.read_csv('/content/drive/MyDrive/UT AI ML PG program/Module 2_Machine Learning/Project/Loan_Modelling.csv')

#copying the dataset to another variable to avoid changes to the original dataset
df = data.copy()

"""## Data Overview

* Observations
* Sanity checks

### Viewing the first and last 5 rows of the dataset
"""

#checking the first 5 rows of the dataset
df.head()

#checking the last 5 rows of the dataset
df.tail()

"""###Checking the shape of the dataset"""

#shape of the dataset
df.shape

"""The dataset has 5000 rows and 14 columns.

###Checking the attribute types
"""

#datatypes of the columns of the dataset
df.info()

"""The dataset contains 14 original columns, comprising both numerical and categorical variables. Numerical features included income, age, family size, and credit card usage. Categorical features included education level and ZIPCode, which are one-hot encoded to avoid issues with interpretation in the model. Two variables — Experience and Personal_Loan — are removed from the feature matrix before modeling.

###Checking the statistical summary
"""

#descriptive statistics of the dataset
df.describe().T

"""1. Class Imbalance in Target Variable (Personal_Loan)
- Only ~9.6% of customers accepted the personal loan (mean = 0.096)
- Indicates a highly imbalanced classification problem
- Suggests the need to use Recall , Precision , and F1-score instead of Accuracy for model evaluation
2. Income Distribution is Skewed
- Mean income: ~73.77K dollars
- Max income: 224K dollars
- Standard deviation: ~46K dollars — shows wide variation in income levels
- Customers earning above 98K dollars (75th percentile) are more likely to accept loans
3. CCAvg (Credit Card Spending) Shows High Variability
- Average spending: ~1.93K dollars/month
- Max: 10K dollars/month
- Standard deviation: ~1.76K dollars — indicates right-skewed distribution
- Top spenders could be potential loan candidates
4. Most Customers Don't Have Mortgage or Securities Account
- Median values for Mortgage, Securities_Account, and CD_Account are all zero
- Majority of customers do not have these financial products
- However, those who do may represent higher-income or higher-risk groups
5. Family Size and Age Are Fairly Normally Distributed
- Mean age: ~45 years
- Mean family size: ~2.4 members
- Both variables show symmetric spread , with median close to mean
- Indicates balanced demographic coverage across customers

###Checking for null values
"""

# checking for null values
data.isnull().sum()

"""There are no null values.

###Checking for duplicate values
"""

# checking for duplicate values
data.duplicated().sum()

"""There are no duplicated values.

##**Data Preprocessing**

###Checking for anomalous values
"""

#inspect the unique values present in the 'Experience' column
data["Experience"].unique()

"""There are some negative values which can cause inconsistencies."""

# checking for experience <0
data[data["Experience"] < 0]["Experience"].unique()

# Correcting the experience values
data["Experience"].replace(-1, 1, inplace=True)
data["Experience"].replace(-2, 2, inplace=True)
data["Experience"].replace(-3, 3, inplace=True)

#inspect the unique values in the 'Education' column
data["Education"].unique()

"""The negative values were replaced by positive values successfully. Anomalies in both 'Education' and 'Experience' columns have been corrected.

###Feature Engineering
"""

# checking the number of uniques in the zip code
data["ZIPCode"].nunique()

"""There are 467 unique values in ZIPCode column."""

#process the 'ZIPCode' column in the dataset
data["ZIPCode"] = data["ZIPCode"].astype(str) #convert the data type of the 'ZIPCode' column to a string
#print the number of unique values if only the first two digits of the ZIP code are considered
print(
    "Number of unique values if we take first two digits of ZIPCode: ",
    data["ZIPCode"].str[0:2].nunique(),
)
data["ZIPCode"] = data["ZIPCode"].str[0:2] #update the 'ZIPCode' column to contain only the first two digits of the original ZIP codes

data["ZIPCode"] = data["ZIPCode"].astype("category") #convert the data type of the modified 'ZIPCode' column to category

"""There are only 7 unique ZIP code regions when considering just the first two digits
This reduces dimensionality and helps group customers by geographic clusters which is useful for identifying regional patterns in loan acceptance behavior.
"""

## Converting the data type of categorical features to 'category'
cat_cols = [
    "Education",
    "Personal_Loan",
    "Securities_Account",
    "CD_Account",
    "Online",
    "CreditCard",
    "ZIPCode",
]
data[cat_cols] = data[cat_cols].astype('category')

"""- Original ZIP codes had 467 unique values
- After transformation, only 7 unique ZIP code regions remain
- Makes the feature more manageable and interpretable

##**Exploratory Data Analysis**

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Questions**:

1. What is the distribution of mortgage attribute? Are there any noticeable patterns or outliers in the distribution?
2. How many customers have credit cards?
3. What are the attributes that have a strong correlation with the target attribute (personal loan)?
4. How does a customer's interest in purchasing a loan vary with their age?
5. How does a customer's interest in purchasing a loan vary with their education?

###Univariate Analysis
"""

#define function to create histogram and boxplot
def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to show the density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

# function to create labeled barplots


def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n].sort_values(),
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

#boxplot and histogram of age
histogram_boxplot(data, "Age")

"""The histogram shows a bell-shaped curve , indicating an approximately normal distribution of age"""

#boxplot and histogram of experience
histogram_boxplot(data, "Experience")

"""- Distribution is fairly symmetric — median ≈ mean (~20 years)
- Some negative values were present and corrected during preprocessing
- No significant outliers after correction
- Strongly correlated with Age → dropped before modeling
- Insight: Experience isn’t a strong predictor once Age is included.
"""

#boxplot and histogram of income
histogram_boxplot(data, "Income")

"""- Mean income: ~73.77K USD
- Standard deviation: ~46K USD — wide variation in income levels
- Right-skewed distribution — few high-income individuals
- Top 25% earn more than $98K/year
- Key Predictor: Income showed strongest correlation with Personal_Loan

Income exhibit right-skewed distribution.
"""

#boxplot and histogram of CCAvg
histogram_boxplot(data, "CCAvg")

"""- Average spending: ~1.93K USD/month
- Median = $1.5K, Max = 10K USD — indicates right-skewness
- High variability among customers
- Customers who accepted loans had higher average CCAvg
- Insight: CCAvg is a good proxy for financial behavior and borrowing comfort
"""

#boxplot and histogram of Mortgage
histogram_boxplot(data, "Mortgage")

"""- Outliers: Values above ~$207K are considered outliers
- Interpretation: Mortgage alone isn't a strong indicator, but may indicate wealth or debt burden when combined with other features
"""

#Barplot of family
labeled_barplot(data, "Family", perc=True)

"""Most customers have family size 1–3; those with family size 4 are less likely to accept loans"""

#Barplot of education
labeled_barplot(data, 'Education', perc=True)

"""Graduate & Advanced degree holders (~28% of dataset) are more likely to accept personal loans than undergrads"""

##Barplot of Securities_Account
labeled_barplot(data, 'Securities_Account', perc=True)

"""Only ~10.4% of customers have this account; doesn’t strongly influence loan acceptance"""

#Barplot of CD_Account
labeled_barplot(data, 'CD_Account', perc=True)

"""6% of customers have CD accounts; these customers are slightly more likely to accept personal loans"""

##Barplot of Online
labeled_barplot(data, 'Online', perc=True)

"""~60% of customers use online banking; digital users show slightly better conversion rates"""

#Barplot of CreditCard
labeled_barplot(data, 'CreditCard', perc=True)

"""~29.4% of customers use credit cards from other banks; they are slightly less likely to accept AllLife’s personal loan"""

#Barplot of ZIPCode
labeled_barplot(data, 'ZIPCode', perc=True)

"""After reducing to first two digits, only 7 unique regions remain; some ZIP codes (like 92, 94) show higher acceptance rates

###Bivariate Analysis
"""

#define a function to generate stacked bar chart of a predictor variable and a target variable
def stacked_barplot(data, predictor, target):
    """
    Print the category counts and plot a stacked bar chart

    data: dataframe
    predictor: independent variable
    target: target variable
    """
    count = data[predictor].nunique() #counting unique values
    sorter = data[target].value_counts().index[-1] #determine sort order
    #create a cross-tabulation table with margins
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(
        by=sorter, ascending=False
    )
    print(tab1)
    print("-" * 120)

    #create a normalized cross-tabulation table
    tab = pd.crosstab(data[predictor], data[target], normalize="index").sort_values(
        by=sorter, ascending=False
    )
    tab.plot(kind="bar", stacked=True, figsize=(count + 5, 5)) #plotting the stacked bar chart
    plt.legend(
        loc="lower left", frameon=False,
    )
    plt.legend(loc="upper left", bbox_to_anchor=(1, 1))
    plt.show() #displays the plot

### Function to visualize the distribution of a predictor variable with respect to the target variable ###

def distribution_plot_wrt_target(data, predictor, target):
    """
    Plots to visualize the relationship between a predictor and the target (binary classification)

    Parameters:
    data (DataFrame): The input dataframe containing the dataset
    predictor (str): Name of the predictor (independent) variable
    target (str): Name of the target (dependent) variable (assumed binary)

    Returns:
    None: Displays plots only
    """

    # Create a figure with 2x2 subplots for visualizing different aspects of the data
    fig, axs = plt.subplots(2, 2, figsize=(12, 10))

    # Get unique values in the target column (assumes binary classification: e.g., 0 and 1)
    target_uniq = data[target].unique()

    # Histogram for the first class of the target (e.g., target = 0)
    axs[0, 0].set_title("Distribution of target for target=" + str(target_uniq[0]))
    sns.histplot(
        data=data[data[target] == target_uniq[0]],  # Filter data for target=0
        x=predictor,                                # Predictor variable on x-axis
        kde=True,                                   # Overlay kernel density estimate
        ax=axs[0, 0],                               # Top-left subplot
        color="teal",                               # Color for this plot
        stat="density",                             # Normalize histogram to probability density
    )

    # Histogram for the second class of the target (e.g., target = 1)
    axs[0, 1].set_title("Distribution of target for target=" + str(target_uniq[1]))
    sns.histplot(
        data=data[data[target] == target_uniq[1]],  # Filter data for target=1
        x=predictor,
        kde=True,
        ax=axs[0, 1],                               # Top-right subplot
        color="orange",
        stat="density",
    )

    # Boxplot showing distribution of predictor by target class
    axs[1, 0].set_title("Boxplot w.r.t target")
    sns.boxplot(
        data=data,
        x=target,              # Categorical grouping variable
        y=predictor,           # Numerical variable to plot
        ax=axs[1, 0],          # Bottom-left subplot
        palette="gist_rainbow" # Color palette
    )

    # Boxplot without outliers to better understand central tendency
    axs[1, 1].set_title("Boxplot (without outliers) w.r.t target")
    sns.boxplot(
        data=data,
        x=target,
        y=predictor,
        ax=axs[1, 1],
        showfliers=False,      # Do not display outliers
        palette="gist_rainbow"
    )

    # Adjust spacing between subplots for better readability
    plt.tight_layout()
    plt.show()

#create a heatmap to visualize the correlation matrix
plt.figure(figsize=(15, 7))
sns.heatmap(df.corr(numeric_only=True), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()

"""- Income , CCAvg , and Family size are top predictors, income being the biggest predictor.
- Other variables can be used for segmentation or encoding, but not primary drivers.
"""

#create stacked barplot of education vs. personal loan and print cross-tabulation table
stacked_barplot(data, "Education", "Personal_Loan")

"""Undergrad (4.4%) < Graduate (13.0%) < Advanced (13.7%)"""

#create stacked barplot of family vs. personal loan and print cross-tabulation table
stacked_barplot(data, 'Family', 'Personal_Loan')

"""Customers with family size = 3 had the highest likelihood of accepting a personal loan"""

##create stacked barplot of Securities_Account vs. personal loan and print cross-tabulation table
stacked_barplot(data, 'Securities_Account', 'Personal_Loan')

"""Slight difference — securities holders have slightly lower acceptance rate"""

#create stacked barplot of CD_Account vs. personal loan and print cross-tabulation table
stacked_barplot(data, 'CD_Account', 'Personal_Loan')

"""
CD account holders are more likely to accept loans (~140 out of 302 accepted)"""

#create stacked barplot of Online vs. personal loan and print cross-tabulation table
stacked_barplot(data, 'Online', 'Personal_Loan')

"""
Online users are more likely to accept (~9.7% vs ~6.2%)"""

#create stacked barplot of CreditCard vs. personal loan and print cross-tabulation table
stacked_barplot(data, 'CreditCard', 'Personal_Loan')

"""Credit card users from other banks are slightly less likely to accept"""

#create stacked barplot of ZIPCode vs. personal loan and print cross-tabulation table
stacked_barplot(data, 'ZIPCode', 'Personal_Loan')

"""
Certain ZIP codes (e.g., 92, 94) show significantly higher conversion rates"""

#visualize the distribution of age in relation to personal loan
distribution_plot_wrt_target(data, 'Age', 'Personal_Loan')

"""- Overlapping distributions between acceptors and non-acceptors
- Mean age of both groups is around ~45 years
- Older customers (>60) show slightly lower interest
- Conclusion: Age alone is not a strong driver of loan acceptance
"""

#visualize the distribution of experience in relation to personal loan
distribution_plot_wrt_target(data, 'Experience', 'Personal_Loan')

"""- Similar pattern to Age — overlapping distributions
- Negative values were cleaned early in preprocessing
- Experience was dropped due to redundancy with Age
- Good move: Removing redundant features helps simplify the model
"""

#visualize the distribution of income in relation to personal loan
distribution_plot_wrt_target(data, 'Income', 'Personal_Loan')

"""- Acceptors clearly have higher income than non-acceptors
- Majority of acceptors earn over $92.5K/year
- Business Insight: Income is the best filter for identifying potential customers
"""

#visualize the distribution of CCAvg in relation to personal loan
distribution_plot_wrt_target(data, 'CCAvg', 'Personal_Loan')

"""- Acceptors spend more on credit cards
- High CC spenders are more likely to accept personal loans
- Marketing Tip: Target customers who already manage revolving debt responsibly

###Outlier Detection
"""

Q1 = data.select_dtypes(include=["float64", "int64"]).quantile(0.25)  # find the 25th percentile and 75th percentile.
Q3 = data.select_dtypes(include=["float64", "int64"]).quantile(0.75)

IQR = Q3 - Q1  # Inter Quantile Range (75th perentile - 25th percentile)

lower = (
    Q1 - 1.5 * IQR
)  # Finding lower and upper bounds for all values. All values outside these bounds are outliers
upper = Q3 + 1.5 * IQR

#calculate the percentage of data points that are considered outliers in the numerical columns
(
    (data.select_dtypes(include=["float64", "int64"]) < lower)
    | (data.select_dtypes(include=["float64", "int64"]) > upper)
).sum() / len(data) * 100

"""The dataset was analyzed for outliers using the Interquartile Range (IQR) method. Most variables such as Age, Experience, and Family showed no outliers. However, Income, CCAvg, and Mortgage had notable outlier percentages — 1.92%, 6.48%, and 5.82% respectively .

These outliers were expected in financial attributes and do not necessarily represent invalid data points. For this project, since we're using a Decision Tree classifier , which is robust to outliers, we proceeded without removing or capping these values.

**Questions**:

1. What is the distribution of mortgage attribute? Are there any noticeable patterns or outliers in the distribution?
- The Mortgage column shows a highly skewed distribution (towards right).
- Most customers have no mortgage at all (zero value).
- A small number of customers have very high mortgage values (up to $635K), and values above ~$207K can be considered outliers using IQR method.

2. How many customers have credit cards?
- About 29.4% of customers (1,470 out of 5,000) use credit cards from other banks.

3. What are the attributes that have a strong correlation with the target attribute (personal loan)?
- Income has the strongest positive correlation with loan acceptance.
- CCAvg (credit card usage) suggests financial activity and comfort with debt.
- Family size = 3 showed highest likelihood of accepting a loan.
- ZIPCode, Age, and Experience had negligible impact.

4. How does a customer's interest in purchasing a loan vary with their age?
- Age alone is not a strong predictor of personal loan acceptance.
- There is no significant difference between acceptors and non-acceptors by age.
- This aligns with correlation analysis where Age had near-zero correlation.

5. How does a customer's interest in purchasing a loan vary with their education?
- Graduate and advanced degree holders are more than 3x more likely to accept personal loans than undergraduates.

###Data Preparation for Modeling
"""

# Drop the 'Experience' column as it is highly correlated with 'Age', and keep only relevant features for modeling
X = data.drop(["Personal_Loan", "Experience"], axis=1)

# Define the target variable: we are predicting whether a customer will accept a personal loan
Y = data["Personal_Loan"]

# Perform one-hot encoding on categorical variables 'ZIPCode' and 'Education'
# Using drop_first=True to avoid multicollinearity by dropping one category from each feature
X = pd.get_dummies(X, columns=["ZIPCode", "Education"], drop_first=True)
# Convert all feature columns to float type to ensure compatibility with machine learning models
X = X.astype(float)

# Split the dataset into training (70%) and testing (30%) sets
# Setting random_state=1 ensures reproducibility of the split
X_train, X_test, y_train, y_test = train_test_split(
    X, Y, test_size=0.30, random_state=1
)

# Display the shape (number of rows and features) of the training and test sets
print("Shape of Training set : ", X_train.shape)
print("Shape of test set : ", X_test.shape)

# Show the class distribution in the training set as percentages
# This helps identify if there's class imbalance
print("Percentage of classes in training set:")
print(y_train.value_counts(normalize=True))

# Show the class distribution in the test set as percentages
# Ensures that the class distribution is similar between train and test sets
print("Percentage of classes in test set:")
print(y_test.value_counts(normalize=True))

"""Before proceeding with model building, the dataset was divided into training and test sets using a 70:30 split to ensure sufficient data for both training and validation.

Shape of Data
- Training set : 3500 samples with 18 features
- Test set : 1500 samples with 18 features
- This indicates that the feature matrix has been successfully prepared and is ready for modeling.

- The dataset is highly imbalanced , with only about ~9.5% of customers accepting the personal loan
- The class distribution is similar between training and test sets , ensuring that the evaluation results will be representative of the overall population
- Since the minority class (Personal_Loan = 1) is the focus of this project (identifying potential loan customers), Recall and F1-score were prioritized over Accuracy during model evaluation

##**Model Building**

### Model Evaluation Criterion

###**Model Evaluation Criteria with Proper Reasoning**

In this classification task (predicting whether a customer will accept a **Personal Loan**), the following **model evaluation metrics** were used:

---

### 1. **Accuracy**
- **Definition**: Proportion of total correct predictions (both true positives and true negatives) out of all predictions.
- **Why it's useful**: Gives an overall idea of model performance.
- **Limitation**: Can be misleading in **imbalanced datasets**, where one class dominates.

> In our dataset, `Personal_Loan` is likely imbalanced (fewer people accept loans than decline), so accuracy alone isn't sufficient.

---

### 2. **Recall (Sensitivity / True Positive Rate)**
- **Definition**: Proportion of actual positive cases that are correctly identified.
- **Formula**: Recall = TP / (TP + FN)
- **Why it's important**: We want to minimize **false negatives** — i.e., customers who would have accepted a loan but were predicted not to.
- **Business Impact**: Missing potential loan takers could mean lost revenue for the bank.

> **Used as the main metric during hyperparameter tuning and post-pruning**, because identifying all possible loan applicants is crucial.

---

### 3. **Precision**
- **Definition**: Proportion of predicted positive cases that are actually positive.
- **Formula**: Precision = TP / (TP + FP)
- **Why it's important**: Helps reduce **false positives** — i.e., targeting people who won’t take the loan.
- **Business Impact**: Reduces unnecessary marketing costs by avoiding false positives.

---

### 4. **F1-Score**
- **Definition**: Harmonic mean of precision and recall.
- **Why it's important**: Balances both precision and recall — especially useful when dealing with **class imbalance**.
- **Use Case**: When you need a **single number to compare models** fairly across different configurations.

---

### 5. **Confusion Matrix**
- **Definition**: A matrix showing counts of:
  - **True Positives (TP)**  
  - **True Negatives (TN)**  
  - **False Positives (FP)**  
  - **False Negatives (FN)**
- **Why it's important**: Provides granular insight into types of errors made by the model.
- **Business Use**: Helps understand cost implications:
  - **False Negative (missed opportunity)**
  - **False Positive (wasted effort)**

---

## Summary Table: Why These Metrics Matter

| Metric             | Why Used |
|--------------------|----------|
| **Accuracy**       | Overall correctness (good for quick checks) |
| **Recall**         | Minimize missed loan opportunities |
| **Precision**      | Avoid targeting uninterested customers |
| **F1-score**       | Balanced measure for imbalanced data |
| **Confusion Matrix** | Understand error types and business impact |

---
"""

# Define a function to evaluate the performance of a classification model
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    Parameters:
    - model: trained classifier (e.g., DecisionTreeClassifier, LogisticRegression)
    - predictors: feature matrix (X_train or X_test)
    - target: actual labels (y_train or y_test)

    Returns:
    - df_perf: DataFrame containing Accuracy, Recall, Precision, and F1-score
    """

    # Make predictions using the input model on the given predictors
    pred = model.predict(predictors)

    # Calculate accuracy: proportion of correct predictions (both true positives and true negatives)
    acc = accuracy_score(target, pred)

    # Calculate recall: proportion of actual positives correctly identified
    # Also known as sensitivity or true positive rate
    recall = recall_score(target, pred)

    # Calculate precision: proportion of predicted positives that are actually positive
    # Measures how many selected items are relevant
    precision = precision_score(target, pred)

    # Calculate F1-score: harmonic mean of precision and recall
    # Useful when you want to balance precision and recall, especially with class imbalance
    f1 = f1_score(target, pred)

    # Create a DataFrame to store and return all the performance metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],  # Single-row DataFrame
    )

    return df_perf

def confusion_matrix_sklearn(model, predictors, target):
    """
    Function to plot a confusion matrix with both counts and percentages for classification models

    Parameters:
    - model: trained classifier (e.g., DecisionTreeClassifier)
    - predictors: feature matrix (X_train or X_test)
    - target: actual labels (y_train or y_test)
    """

    # Predict target values using the model and given predictors
    y_pred = model.predict(predictors)

    # Compute the confusion matrix using sklearn's confusion_matrix
    cm = confusion_matrix(target, y_pred)

    # Create custom labels that include both the count and the percentage of total samples
    # Format: "count\n(percentage)"
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)  # Reshape to match the 2x2 structure of the confusion matrix

    # Set up the plot figure size
    plt.figure(figsize=(6, 4))

    # Plot the confusion matrix as a heatmap
    # annot=labels displays both count and percentage in each cell
    sns.heatmap(cm, annot=labels, fmt="", cmap="Blues")  # fmt="" to use string-formatted labels

    # Add axis labels
    plt.ylabel("True label")
    plt.xlabel("Predicted label")

    # Optional: Add title
    plt.title("Confusion Matrix")

    # Display the plot
    plt.show()

"""###Decision Tree (sklearn default)"""

# Initialize a Decision Tree Classifier with Gini impurity as the splitting criterion
# 'criterion="gini"' means the model will use the Gini Index to measure node purity
# 'random_state=1' ensures reproducibility of the results
model = DecisionTreeClassifier(criterion="gini", random_state=1)

# Train the model using the training sets (X_train as features, y_train as target)
# This step involves building the tree by splitting nodes based on feature values to minimize impurity
model.fit(X_train, y_train)

"""- Gini Impurity measures how often a randomly chosen element would be incorrectly classified if labeled based on the distribution of classes in the subset.
- A lower Gini value indicates a better split — useful for imbalanced datasets like this one.

###Checking model performance on training data
"""

# Generate and visualize the confusion matrix for the training data
# This shows how well the trained model performs on the training set
confusion_matrix_sklearn(model, X_train, y_train)

"""- The model has perfect recall , which means it successfully identifies all customers who will accept the loan
- It also has perfect precision , meaning it doesn’t mistakenly label anyone as a loan taker
- This makes it ideal for targeting strategies , where missing a true loan candidate is more costly than targeting a few extra people
"""

# Evaluate the performance of the trained Decision Tree model on the training data
# The function returns key classification metrics: Accuracy, Recall, Precision, and F1-score
decision_tree_perf_train = model_performance_classification_sklearn(model, X_train, y_train)

# Display the performance metrics DataFrame
decision_tree_perf_train

"""- The model achieved 100% accuracy, recall, precision, and F1-score on the training data.
- This means the model has memorized the training data perfectly
- Good side : It learned all patterns in training data
- Bad side : Strong sign of overfitting

###Visualizing the decision tree
"""

# Extract and store the names of the features (columns) from the training data
feature_names = list(X_train.columns)

# Print the list of feature names to verify which variables are being used in the model
print(feature_names)

"""- The model uses 18 features in total
- These include both original numeric features and dummy variables created from ZIPCode and Education after one-hot encoding
- Most important predictors like Income , Family , and CCAvg are included
- Features like ZIPCode_92 , Education_2 , etc., were encoded using drop_first=True, reducing multicollinearity
"""

# Set the figure size for better readability of the plotted decision tree
plt.figure(figsize=(20, 30))

# Plot the structure of the decision tree
out = tree.plot_tree(
    model,                     # Trained Decision Tree model
    feature_names=feature_names,  # Names of features used in the model
    filled=True,              # Color nodes by majority class
    fontsize=9,               # Font size for text inside the nodes
    node_ids=False,           # Do not display node IDs
    class_names=None,         # Do not show class names in leaf nodes (set to True for classification)
)

# The following loop adds arrows between nodes if they are missing or not properly styled
for o in out:
    arrow = o.arrow_patch
    if arrow is not None:
        arrow.set_edgecolor("black")   # Set arrow color to black
        arrow.set_linewidth(1)         # Set arrow thickness

# Display the final plot
plt.show()

"""The tree visualization showed a deep structure with multiple levels of branching."""

# Generate and print a textual representation of the decision tree rules
# This shows the decision path in an easy-to-read format

print(
    tree.export_text(              # Function to export tree structure as text
        model,                     # Trained Decision Tree model
        feature_names=feature_names,  # Names of features used in the model
        show_weights=True          # Show the number of samples (weights) at each node
    )
)

"""The tree makes decisions starting from top predictors like Income, then adds detail using Family, Education, etc."""

# Display the importance of each feature in the Decision Tree model
# Feature importance indicates how much each feature contributes to the decision-making process
# It is calculated as the normalized total reduction of the Gini impurity (or variance for regression) brought by that feature

print(
    pd.DataFrame(
        model.feature_importances_,   # Extracts the feature importance values from the trained model
        columns=["Imp"],              # Name the importance column as 'Imp'
        index=X_train.columns         # Use training data column names as row index (feature names)
    ).sort_values(by="Imp", ascending=False)  # Sort features by importance in descending order
)

"""- Income is the most influential feature.
- Family size and education level play a major role.
- Credit card spending is moderately important.
- ZIPCode dummies and other flags are not useful.
"""

# Extract feature importances from the trained model
importances = model.feature_importances_

# Get the indices of features sorted by their importance (ascending order for plotting)
indices = np.argsort(importances)

# Create a horizontal bar chart to visualize feature importances
plt.figure(figsize=(8, 8))
plt.title("Feature Importances")  # Title of the plot

# Plot a horizontal bar chart:
# - y-axis: feature names
# - x-axis: corresponding importance scores
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")

# Set the y-tick labels to feature names, sorted by importance
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])

# Label the x-axis
plt.xlabel("Relative Importance")

# Display the plot
plt.show()

"""- Income is the most important predictor — it dominates the decision-making process.
- Family size is the second most important, indicating that demographics play a key role.
- Education level also matters, especially for graduate/professional customers.
- Low-Importance Features:
Many features like ZIPCode, Mortgage, CreditCard, etc., have negligible importance.

###Checking model performance on test data
"""

# Generate and visualize the confusion matrix for the test data
# This evaluates how well the trained model performs on unseen (test) data
confusion_matrix_sklearn(model, X_test, y_test)

"""- The model correctly classified 1342 out of 1500 test samples , resulting in an accuracy of approximately 89.47%.
- This indicates that the model generalizes reasonably well to unseen data.
- The model successfully identified all actual loan acceptors without missing any — no false negatives.
- While recall is perfect, precision is slightly lower due to some false positives (customers predicted to accept loans but did not).
- No actual loan acceptors were missed, confirming high recall.
"""

# Evaluate the performance of the trained Decision Tree model on the test data
# The function returns key classification metrics: Accuracy, Recall, Precision, and F1-score
decision_tree_perf_test = model_performance_classification_sklearn(model, X_test, y_test)

# Display the performance metrics DataFrame for the test set
decision_tree_perf_test

"""1. High Accuracy (98.3%):
- The model correctly classified 98.3% of the test samples.
- Indicates strong overall performance.
2. Perfect Recall (89.26%):
- The model successfully identified 89.26% of actual loan acceptors without missing any.
- Since the target variable (Personal_Loan) is imbalanced (~9% positive class), recall is crucial to ensure no potential loan customers are overlooked.
3. Good Precision (93.66%):
- 93.66% of predicted loan acceptors were correct , meaning the model doesn't over-predict too many false positives.
- This ensures marketing efforts are focused on customers who are more likely to accept loans.
4. Balanced F1-Score (91.41%):
- The F1-score , which balances precision and recall, is 0.914089.
- This indicates a good trade-off between identifying all potential loan customers and avoiding unnecessary targeting.

## Model Performance Improvement

###Pre-pruning
"""

# Define a range of hyperparameters to test for pre-pruning the Decision Tree
# These parameters control tree complexity and help prevent overfitting

max_depth_values = np.arange(2, 7, 2)          # Tree depth: controls how deep the tree can grow
max_leaf_nodes_values = [50, 75, 150, 250]     # Max number of leaf nodes
min_samples_split_values = [10, 30, 50, 70]    # Minimum samples required to split an internal node

# Initialize variables to keep track of the best model found during the search
best_estimator = None                          # Stores the best model
best_score_diff = float('inf')                 # Stores smallest difference between train and test recall
best_test_score = 0.0                          # Best test recall score

# Nested loops to iterate through all combinations of hyperparameters
for max_depth in max_depth_values:
    for max_leaf_nodes in max_leaf_nodes_values:
        for min_samples_split in min_samples_split_values:

            # Instantiate the Decision Tree Classifier with current hyperparameters
            estimator = DecisionTreeClassifier(
                max_depth=max_depth,              # Maximum depth of the tree
                max_leaf_nodes=max_leaf_nodes,    # Maximum number of leaf nodes
                min_samples_split=min_samples_split,  # Minimum samples needed to split a node
                class_weight='balanced',          # Adjust weights inversely proportional to class frequencies
                random_state=42                   # Ensures reproducibility
            )

            # Train the model on the training data
            estimator.fit(X_train, y_train)

            # Make predictions on both training and test sets
            y_train_pred = estimator.predict(X_train)
            y_test_pred = estimator.predict(X_test)

            # Evaluate Recall (important metric due to possible class imbalance)
            train_recall_score = recall_score(y_train, y_train_pred)
            test_recall_score = recall_score(y_test, y_test_pred)

            # Compute the absolute difference between train and test recall
            score_diff = abs(train_recall_score - test_recall_score)

            # Update the best model if this one has better generalization:
            # - smaller difference between train and test recall (less overfitting)
            # - higher test recall score
            if (score_diff < best_score_diff) & (test_recall_score > best_test_score):
                best_score_diff = score_diff
                best_test_score = test_recall_score
                best_estimator = estimator

# Print out the best hyperparameters and corresponding performance
print("Best parameters found:")
print(f"Max depth: {best_estimator.max_depth}")
print(f"Max leaf nodes: {best_estimator.max_leaf_nodes}")
print(f"Min samples split: {best_estimator.min_samples_split}")
print(f"Best test recall score: {best_test_score}")

"""The optimal hyperparameters found were:

- Max depth : 2
- Max leaf nodes : 50
- Min samples split : 10
- The best model achieved a test recall score of 1.0, indicating perfect identification of all actual loan acceptors in the test set.
"""

# Assign the best model found during hyperparameter tuning to 'estimator'
estimator = best_estimator

# Re-train the best model on the full training dataset
# This ensures the model learns from all available training data before evaluation or deployment
estimator.fit(X_train, y_train)

"""###Checking model performance on training data"""

# Generate and visualize the confusion matrix for the best model on the training data
# This helps evaluate how well the tuned model performs on the data it was trained on
confusion_matrix_sklearn(estimator, X_train, y_train)

"""1. True Negatives (TN) : 2435
- Correctly identified non-loan acceptors .
- Represents 69.57% of the total training samples.
2. False Positives (FP) : 734
- Incorrectly predicted loan acceptance for customers who did not accept loans.
- Represents 20.97% of the total training samples.
3. False Negatives (FN) : 0
- No actual loan acceptors were missed by the model.
- Indicates perfect recall on the training data.
4. True Positives (TP) : 331
- Correctly identified loan acceptors .
- Represents 9.46% of the total training samples.
"""

# Evaluate the performance of the best (tuned) Decision Tree model on the training data
# This will give key classification metrics like Accuracy, Recall, Precision, and F1-score

decision_tree_tune_perf_train = model_performance_classification_sklearn(estimator, X_train, y_train)

# Display the resulting performance metrics DataFrame for the training set
decision_tree_tune_perf_train

"""- Recall = 1.0: The model successfully captured all actual loan acceptors , ensuring no missed opportunities.
- Precision = 0.31: While recall is perfect, precision is relatively low due to a high number of false positives.
- Accuracy = 0.79: Overall correctness is reasonable but slightly lower than expected given the imbalanced dataset.
- F1-score = 0.47: Balances recall and precision, indicating moderate performance.

###Visualizing Decision Tree
"""

# Set the size of the plot for better readability
plt.figure(figsize=(10, 10))

# Plot the structure of the best (tuned) Decision Tree model
out = tree.plot_tree(
    estimator,                # The best Decision Tree model after hyperparameter tuning
    feature_names=feature_names,  # Names of features used in the model
    filled=True,              # Color nodes by class or value distribution
    fontsize=9,               # Font size inside each node
    node_ids=False,           # Do not display node IDs
    class_names=None,         # Do not show class names in leaf nodes (set to True for classification)
)

# Enhance the appearance of arrows between nodes
for o in out:
    arrow = o.arrow_patch
    if arrow is not None:
        arrow.set_edgecolor("black")   # Make the arrows black for better visibility
        arrow.set_linewidth(1)         # Set a consistent line width for all arrows

# Display the final plot
plt.show()

"""The visualization of the tuned Decision Tree model provides a clear understanding of how the model makes decisions to predict whether a customer will accept a personal loan."""

# Generate and print a textual representation of the decision tree rules
# This shows the decision path in an easy-to-read format

print(
    tree.export_text(              # Function to export tree structure as text
        estimator,                 # Best Decision Tree model after tuning
        feature_names=feature_names,  # Names of features used in the model
        show_weights=True          # Show the number of samples (weights) at each node
    )
)

# Display the importance of each feature in the best Decision Tree model
# Feature importance indicates how much each feature contributes to the decision-making process
# It is calculated as the normalized total reduction of the Gini impurity (or variance for regression) brought by that feature

print(
    pd.DataFrame(
        estimator.feature_importances_,   # Extracts feature importance values from the tuned model
        columns=["Imp"],                  # Name the importance column as 'Imp'
        index=X_train.columns             # Use training data column names as row index (feature names)
    ).sort_values(by="Imp", ascending=False)  # Sort features by importance in descending order
)

# Extract feature importances from the best (tuned) Decision Tree model
importances = estimator.feature_importances_

# Get the indices that would sort the feature importances in ascending order
indices = np.argsort(importances)

# Create a new figure for plotting with specified size
plt.figure(figsize=(8, 8))

# Set the title of the plot
plt.title("Feature Importances")

# Plot a horizontal bar chart showing feature importances
# The length of each bar corresponds to the importance of the feature
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")

# Set the y-axis tick labels to the corresponding feature names, sorted by importance
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])

# Label the x-axis
plt.xlabel("Relative Importance")

# Display the plot
plt.show()

"""- Income dominates the decision-making process, confirming its critical role in determining loan acceptance.
- High-income customers are more likely to accept loans, aligning with business intuition and earlier EDA findings.
- CCAvg (credit card spending) and Family size play supporting roles in refining predictions.
- Customers with higher credit card spending and moderate family sizes are more likely to accept loans.
- Features like Age, Mortgage, ZIPCode, and education levels (Education_2, Education_3) had no impact on the model's performance.

###Checking performance on test data
"""

# Generate and visualize the confusion matrix for the best (tuned) model on the test data
# This evaluates how well the model performs on unseen (test) data

confusion_matrix_sklearn(estimator, X_test, y_test)

"""- Perfect Recall: The model correctly identified all actual loan acceptors (True Positives = 149, False Negatives = 0).
- High False Positives: A significant number of false positives (331) indicates that the model over-predicts loan acceptance.
- No False Negatives: Ensures no potential loan customer was missed, aligning with the business goal.
"""

# Evaluate the performance of the best (tuned) Decision Tree model on the test data
# This will give key classification metrics like Accuracy, Recall, Precision, and F1-score

decision_tree_tune_perf_test = model_performance_classification_sklearn(estimator, X_test, y_test)

# Display the resulting performance metrics DataFrame for the test set
decision_tree_tune_perf_test

"""- Recall = 1.0: The model successfully captured all actual loan acceptors , ensuring no missed opportunities.
- Precision = 0.31: While recall is perfect, precision is relatively low due to a high number of false positives.
- Accuracy = ~78%: Overall correctness is reasonable but slightly lower than expected given the imbalanced dataset.
- F1-score = ~47%: Balances recall and precision, indicating moderate performance.

- Perfect Recall: The model ensures no potential loan customer is overlooked, which is crucial for maximizing revenue opportunities.
- Focus on Recall: This aligns with the business objective of minimizing false negatives to maximize loan acceptance rates.
- High False Positives: The model over-predicts loan acceptance, leading to inefficiencies in marketing efforts.
- Overfitting Concern: Perfect recall on test data suggests some degree of overfitting, which may not generalize well to unseen data.

###Post-pruning
"""

# Initialize a base Decision Tree Classifier with a fixed random state for reproducibility
clf = DecisionTreeClassifier(random_state=1)

# Compute the pruning path using cost-complexity pruning
# This gives us a sequence of 'ccp_alpha' values and corresponding total impurity of leaves
path = clf.cost_complexity_pruning_path(X_train, y_train)

# Extract the effective alphas and corresponding impurities from the pruning path
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# Convert the pruning path data (ccp_alphas and impurities) into a pandas DataFrame
# This allows for easier inspection and analysis of the cost-complexity pruning path

pd.DataFrame(path)

"""1. Overfitting Concern:
- The initial tree (at ccp_alpha = 0) has very low impurity, suggesting overfitting to the training data.
- Post-pruning will help reduce overfitting by simplifying the tree.
2. Balancing Complexity and Impurity:
- Choosing an appropriate ccp_alpha value is crucial to balance model complexity and impurity.
- A higher ccp_alpha leads to simpler trees but may increase impurity, while a lower ccp_alpha keeps the tree complex but risks overfitting.
3. Cost-Complexity Trade-off:
- The pruning path illustrates the trade-off between the cost of adding splits (complexity) and the reduction in impurity.
- Selecting the right ccp_alpha ensures the model is neither too simple nor too complex.
"""

# Create a figure and axis for plotting with a specified size
fig, ax = plt.subplots(figsize=(10, 5))

# Plot effective alpha values vs total impurity of leaves
# Using steps-post drawstyle to emphasize discrete changes between alphas
# Exclude the last alpha (ccp_alphas[-1]) since it collapses the tree to a single node
ax.plot(ccp_alphas[:-1], impurities[:-1], marker="o", drawstyle="steps-post")

# Label the x-axis
ax.set_xlabel("effective alpha")

# Label the y-axis
ax.set_ylabel("total impurity of leaves")

# Set the title of the plot
ax.set_title("Total Impurity vs effective alpha for training set")

# Display the plot
plt.show()

"""1. Key Observations
- The x-axis represents the effective alpha (ccp_alpha) values, which control the trade-off between tree complexity and impurity.
- The y-axis represents the total impurity of leaves, indicating the overall purity of the tree's leaf nodes.
- The plot shows a step-like pattern, where:
- As ccp_alpha increases, the total impurity generally increases.
- At certain points, the impurity remains constant, indicating no change in the tree structure for those ranges of ccp_alpha.
2. Key Features of the Plot:
* Initial Phase (Low ccp_alpha) :
- For small ccp_alpha values (e.g., near 0), the total impurity is low, indicating a complex tree with many splits.
- The impurity increases gradually as ccp_alpha grows slightly, reflecting the removal of some branches or nodes.

* Intermediate Phase :
- Between ccp_alpha ≈ 0.005 and ccp_alpha ≈ 0.025, the impurity remains relatively stable at around 0.03.
- This suggests that within this range, pruning does not significantly affect the tree's structure or impurity.

* Final Phase (High ccp_alpha) :
- Beyond ccp_alpha ≈ 0.025, the impurity increases sharply.
- This indicates aggressive pruning, leading to a simpler tree but higher impurity.

3. Interpretation of Results

a. Effect of ccp_alpha on Impurity
* Low ccp_alpha:
- Corresponds to a more complex tree with lower impurity.
- However, this may lead to overfitting, as the tree becomes too specific to the training data.

* Moderate ccp_alpha:
- Provides a balance between tree complexity and impurity.
- The flat region in the plot (e.g., ccp_alpha ≈ 0.005 to ccp_alpha ≈ 0.025) suggests an optimal range where pruning does not significantly increase impurity.

* High ccp_alpha :
- Leads to excessive pruning, resulting in a simpler tree but higher impurity.
- This may reduce overfitting but could also underfit the data, missing important patterns.

b. Optimal ccp_alpha Selection
- The optimal ccp_alpha can be chosen based on the point where the impurity starts to increase significantly.
- In this case, the flat region between ccp_alpha ≈ 0.005 and ccp_alpha ≈ 0.025 suggests that any value within this range would provide a good trade-off between complexity and impurity.
3. Practical Implications

* Model Complexity:
- Lower ccp_alpha values result in more complex trees, which may overfit the training data.
- Higher ccp_alpha values simplify the tree, reducing overfitting but potentially increasing bias.

* Generalization:
- Choosing an appropriate ccp_alpha ensures the model generalizes well to unseen data by balancing complexity and impurity.

* Decision-Making:
- The plot provides a visual guide for selecting the right level of pruning to achieve the desired trade-off between model complexity and performance.
"""

# Initialize an empty list to store all pruned Decision Tree models
clfs = []

# Loop through each ccp_alpha value from the pruning path
for ccp_alpha in ccp_alphas:
    # Create a Decision Tree Classifier with the current alpha for pruning
    clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)

    # Train the model on the training data
    clf.fit(X_train, y_train)

    # Append the trained model to the list
    clfs.append(clf)

# Print information about the last (most pruned) tree
print(
    "Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
        clfs[-1].tree_.node_count, ccp_alphas[-1]
    )
)

"""- The final tree has just 1 node, meaning it's a single root node — essentially a tree that makes only one decision .
- This suggests the model has been aggressively pruned, possibly reducing it to a constant prediction (e.g., always predicting class 0 or 1).
- Why? A very high ccp_alpha penalizes complexity so heavily that all splits are removed except the root.
- At ccp_alpha = 0.047, the model becomes extremely simple — it stops splitting after the first node.
- This is the last usable tree before full collapse, beyond which even the root node would be pruned.
"""

# Remove the last model (which is fully pruned to a single node)
clfs = clfs[:-1]

# Remove the corresponding ccp_alpha value
ccp_alphas = ccp_alphas[:-1]

# Extract number of nodes for each pruned tree
node_counts = [clf.tree_.node_count for clf in clfs]

# Extract maximum depth for each pruned tree
depth = [clf.tree_.max_depth for clf in clfs]

# Create a figure with two subplots for visualization
fig, ax = plt.subplots(2, 1, figsize=(10, 7))

# Plot alpha vs number of nodes
ax[0].plot(ccp_alphas, node_counts, marker="o", drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")

# Plot alpha vs maximum depth of the tree
ax[1].plot(ccp_alphas, depth, marker="o", drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")

# Adjust layout to prevent overlap
fig.tight_layout()

"""- For small ccp_alpha values (e.g., near 0), the tree is highly complex, with many nodes and deep structure.
- This corresponds to a fully grown tree with minimal pruning.
- Between ccp_alpha ≈ 0.005 and ccp_alpha ≈ 0.025, the tree undergoes significant pruning.
- The number of nodes and depth decrease gradually.
- This range represents a balance between complexity and simplicity.
- Beyond ccp_alpha ≈ 0.025, the tree becomes extremely simple:
- Few nodes and shallow depth.
- Likely results in underfitting due to excessive pruning.
- The optimal ccp_alpha can be chosen based on the point where the tree's complexity starts to stabilize while maintaining good performance on validation data.
- From the plots, ccp_alpha ≈ 0.025 seems like a reasonable threshold, as it balances complexity and simplicity.
- Higher ccp_alpha values reduce overfitting by simplifying the tree, but they may also lead to underfitting if taken too far.
- The goal is to find the sweet spot where the model generalizes well to unseen data.
- Simpler trees (with fewer nodes and lower depth) are easier to interpret, making it clearer which features drive predictions.

###Recall vs. alpha for training and testing sets
"""

# Initialize an empty list to store recall scores for training data
recall_train = []

# Loop through each pruned Decision Tree model
for clf in clfs:
    # Make predictions on the training set
    pred_train = clf.predict(X_train)

    # Calculate recall score for training predictions
    values_train = recall_score(y_train, pred_train)

    # Append the recall score to the list
    recall_train.append(values_train)

# Initialize an empty list to store recall scores for test data
recall_test = []

# Loop through each pruned Decision Tree model
for clf in clfs:
    # Make predictions on the test set
    pred_test = clf.predict(X_test)

    # Calculate recall score for test predictions
    values_test = recall_score(y_test, pred_test)

    # Append the recall score to the list
    recall_test.append(values_test)

# Create a figure and axis object with a specified size
fig, ax = plt.subplots(figsize=(15, 5))

# Set x-axis label
ax.set_xlabel("alpha")

# Set y-axis label
ax.set_ylabel("Recall")

# Set the title of the plot
ax.set_title("Recall vs alpha for training and testing sets")

# Plot recall scores for training data
ax.plot(ccp_alphas, recall_train, marker="o", label="train", drawstyle="steps-post")

# Plot recall scores for test data
ax.plot(ccp_alphas, recall_test, marker="o", label="test", drawstyle="steps-post")

# Add a legend to differentiate between train and test lines
ax.legend()

# Display the final plot
plt.show()

"""- Initially, the recall score is high (~1.0) for low ccp_alpha values.
- As ccp_alpha increases, the recall decreases gradually.
- Beyond a certain point (ccp_alpha ≈ 0.025), the recall stabilizes at a lower value (~0.8).
- Similar to the training set, the recall starts high (~1.0) for low ccp_alpha.
- It decreases more sharply compared to the training set as ccp_alpha increases.
- The recall stabilizes at a lower value (~0.4) for higher ccp_alpha.
- Both training and test recall are high, indicating minimal pruning.
- The model is complex and fits the training data well but may overfit.
- As ccp_alpha increases, both recall scores decrease.
- The test recall drops more steeply than the training recall, indicating reduced generalization ability.
- For very high ccp_alpha, both recall scores stabilize at low values.
- The model becomes overly simplified, leading to underfitting.
- At low ccp_alpha, the model captures all loan acceptors in the training data, resulting in perfect recall.
- As pruning increases, the model simplifies, leading to missed loan acceptors (false negatives).
- Beyond ccp_alpha ≈ 0.025, the recall stabilizes at ~0.8, indicating no further significant improvement or degradation.
- Similar to the training set, the model achieves high recall for low ccp_alpha.
- As pruning increases, the test recall decreases more sharply than the training recall.
- This indicates that the model starts to generalize poorly, missing actual loan acceptors in the test set.
- For high ccp_alpha, the test recall stabilizes at a low value (~0.4), suggesting excessive pruning has led to underfitting.
- Low ccp_alpha values result in high training recall but poor generalization to the test set.
- High ccp_alpha values lead to overly simple models that perform poorly on both training and test data.
- The sweet spot appears around ccp_alpha ≈ 0.025, where both training and test recall stabilize at reasonable levels.
- Lower ccp_alpha values correspond to more complex trees, which fit the training data well but may overfit.
- Higher ccp_alpha values simplify the tree, reducing overfitting but potentially leading to underfitting.
- The goal is to find an ccp_alpha value that balances complexity and generalization.
- From the plot, ccp_alpha ≈ 0.025 seems like a good candidate, as it provides stable recall scores for both training and test data.
- Since the business objective is to minimize false negatives (missed loan acceptors), selecting a model with high recall is crucial.
- While high recall is important, ensuring the model does not overfit is equally critical to maintain performance on unseen data.
"""

# Find the index of the model that achieved the highest recall score on the test data
index_best_model = np.argmax(recall_test)

# Select the best model using that index
best_model = clfs[index_best_model]

# Print the details of the best model, including its ccp_alpha value
print(best_model)

"""- The best model has a ccp_alpha value of 0.000272
- This is a very small alpha, meaning only minor pruning was applied to reduce overfitting while preserving most of the tree's structure.
- A very low ccp_alpha suggests that the model is still relatively complex, but just enough pruning was done to improve generalization slightly.
"""

# Extract the best ccp_alpha value from the best performing pruned model
best_ccp_alpha = best_model.ccp_alpha

# Initialize a new Decision Tree Classifier using the best ccp_alpha for pruning
# class_weight is used to handle class imbalance by giving higher weight to the minority class (1 - loan accepted)
estimator_2 = DecisionTreeClassifier(
    ccp_alpha=best_ccp_alpha,          # Best alpha value found from cross-validation
    class_weight={0: 0.15, 1: 0.85},   # Adjusting weights to handle imbalanced classes
    random_state=1                      # Ensures reproducibility
)

# Train the new model on the full training dataset
estimator_2.fit(X_train, y_train)

"""Checking performance on training data"""

# Generate and visualize the confusion matrix for the final pruned Decision Tree model on the training data
# This helps evaluate how well the model fits the training data after post-pruning

confusion_matrix_sklearn(estimator_2, X_train, y_train)

"""- Perfect Recall: The model correctly identified all actual loan acceptors (True Positives = 331, False Negatives = 0).
- High True Negatives: A large majority of customers who did not accept loans (True Negatives = 3168) were correctly classified.
- No False Negatives: Ensures no potential loan customer was missed, aligning with the business goal.
"""

# Evaluate the performance of the post-pruned Decision Tree model on the training data
# This computes key classification metrics: Accuracy, Recall, Precision, and F1-score

decision_tree_tune_post_train = model_performance_classification_sklearn(estimator_2, X_train, y_train)

# Display the resulting performance metrics DataFrame for the training set
decision_tree_tune_post_train

"""This shows that the post-pruned Decision Tree model performs extremely well on the training set, with all metrics being close to or equal to 1.0.

Visualizing decision tree
"""

# Set the size of the plot for better readability
plt.figure(figsize=(10, 10))

# Plot the structure of the post-pruned Decision Tree
out = tree.plot_tree(
    estimator_2,                # The final pruned Decision Tree model
    feature_names=feature_names,  # Names of features used in the model
    filled=True,                # Color nodes by class or value distribution
    fontsize=9,                 # Font size inside each node
    node_ids=False,             # Do not display node IDs
    class_names=None,           # Do not show class names in leaf nodes (for cleaner look)
)

# Enhance the appearance of arrows between nodes
for o in out:
    arrow = o.arrow_patch
    if arrow is not None:
        arrow.set_edgecolor("black")   # Make the arrows black for better visibility
        arrow.set_linewidth(1)         # Set a consistent line width for all arrows

# Display the final plot
plt.show()

"""The visualization of the post-pruned Decision Tree provides a clear understanding of how the model makes decisions to predict whether a customer will accept a personal loan."""

# Generate and print a textual representation of the post-pruned decision tree rules
# This shows the decision path in an easy-to-read format

print(
    tree.export_text(              # Function to export tree structure as text
        estimator_2,               # The final post-pruned Decision Tree model
        feature_names=feature_names,  # Names of features used in the model
        show_weights=True          # Show the number of samples (weights) at each node
    )
)

# Display the importance of each feature in the post-pruned Decision Tree model
# Feature importance indicates how much each feature contributes to the decision-making process
# It is calculated as the normalized total reduction of the Gini impurity brought by that feature

print(
    pd.DataFrame(
        estimator_2.feature_importances_,   # Extracts feature importance values from the pruned model
        columns=["Imp"],                    # Name the importance column as 'Imp'
        index=X_train.columns               # Use training data column names as row index (feature names)
    ).sort_values(by="Imp", ascending=False)  # Sort features by importance in descending order
)

# Extract feature importances from the post-pruned Decision Tree model
importances = estimator_2.feature_importances_

# Get the indices that would sort the feature importances in ascending order
indices = np.argsort(importances)

# Create a new figure for plotting with specified size
plt.figure(figsize=(8, 8))

# Set the title of the plot
plt.title("Feature Importances")

# Plot a horizontal bar chart showing feature importances
# The length of each bar corresponds to the importance of the feature
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")

# Set the y-axis tick labels to the corresponding feature names, sorted by importance
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])

# Label the x-axis
plt.xlabel("Relative Importance")

# Display the final plot
plt.show()

"""- The feature importance values indicate how much each feature contributes to the decision-making process of the post-pruned Decision Tree model.
- Income has the highest importance score (0.593704), indicating it is the primary driver of loan acceptance predictions.
- Education_2 and CCAvg follow closely, contributing significantly to the model's decisions.
- Education_2 (0.136801) suggests that customers with a specific education level (e.g., Graduate) are more likely to accept loans.
- CCAvg (0.078498) indicates that credit card spending habits also play a role in predicting loan acceptance.
- Education_3 and Family have moderate importance scores (0.066939 and 0.065630, respectively).
- Several features, such as ID, Age, CD_Account, Securities_Account, and Mortgage, have low importance scores.
- ZIP code-related features (ZIPCode_91, ZIPCode_93, etc.) don't contribute at all (0.000000).

Checking performance on test data
"""

# Generate and visualize the confusion matrix for the post-pruned Decision Tree model on the test data
# This evaluates how well the final pruned model performs on unseen (test) data

confusion_matrix_sklearn(estimator_2, X_test, y_test)

"""- The confusion matrix provides a detailed breakdown of how well the post-pruned Decision Tree model performs on the test data, which helps evaluate its generalization capabilities.
- True Positives (TP): The model correctly identified 126 loan acceptors (True Positives = 126).
- False Negatives (FN): There were 23 false negatives, indicating that some actual loan acceptors were missed.
- True Negatives (TN): The model correctly classified 1341 customers who did not accept loans (True Negatives = 1341).
- False Positives (FP): There were 10 false positives, where the model incorrectly predicted loan acceptance.
"""

# Evaluate the performance of the post-pruned Decision Tree model on the test data
# This will give key classification metrics like Accuracy, Recall, Precision, and F1-score

decision_tree_tune_post_test = model_performance_classification_sklearn(estimator_2, X_test, y_test)

# Display the resulting performance metrics DataFrame for the test set
decision_tree_tune_post_test

"""- This shows how well your post-pruned Decision Tree model performs on unseen data, using key classification metrics.
- The post-pruned model achieves strong generalization on test data.
- Compared to the baseline Decision Tree (which overfit), this pruned version strikes a good balance between capturing most loan acceptors (high recall)
and avoiding unnecessary outreach (high precision)

1. High Accuracy = 97.8%
- The model correctly classifies 97.8% of all customers in the test set.
- This indicates strong overall performance, especially considering the dataset is imbalanced (only ~9.5% accepted loans).
- However, accuracy can be misleading in imbalanced datasets — always look at other metrics like recall and precision for a complete picture.

2. Good Recall = 84.56%
- The model identifies 84.56% of actual loan acceptors correctly.
- This is slightly lower than the perfect recall (100%) achieved during training but still very high on test data.
- Why it matters:
- In this business context, missing a potential customer is more costly than falsely targeting someone who won’t convert — so maintaining high recall is crucial.

3. Precision = 92.65%
- Of all customers predicted to accept the loan, 92.65% actually did.
- This suggests that even after pruning, the model maintains high targeting efficiency — not many false positives.
- Business Tip: High precision helps reduce marketing costs by focusing only on likely converters.

4. F1-score = 88.42%
- A harmonic mean of precision and recall
- Balances both types of errors (false positives and false negatives)
- Indicates good overall model quality in an imbalanced setting

## Model Performance Comparison and Final Model Selection
"""

# Combine performance metrics of all three models into a single DataFrame for comparison
models_train_comp_df = pd.concat(
    [
        decision_tree_perf_train.T,       # Baseline Decision Tree (default parameters)
        decision_tree_tune_perf_train.T,  # Pre-pruned Decision Tree (hyperparameter tuned)
        decision_tree_tune_post_train.T   # Post-pruned Decision Tree (cost-complexity pruned)
    ],
    axis=1  # Concatenate along columns
)

# Assign meaningful column names to identify each model
models_train_comp_df.columns = [
    "Decision Tree (sklearn default)",
    "Decision Tree (Pre-Pruning)",
    "Decision Tree (Post-Pruning)"
]

# Print a header for clarity
print("Training performance comparison:")

# Display the final comparison DataFrame
models_train_comp_df

"""###Metrics for training performance comparison
1. Baseline Model:
- Achieves 100% accuracy, 100% recall, and 100% precision.
- The perfect scores suggest overfitting, as it's unlikely to achieve such high metrics on unseen data without simplifying the model.
- While achieving perfect performance on the training set, this model is likely overfitted due to its complexity.
- It may not generalize well to unseen data, making it unsuitable for deployment.
2. Pre-Pruned Model:
- Accuracy: Slightly lower at 0.790286.
- Recall: Maintains 100% recall, ensuring no loan acceptors are missed.
- Precision: Low at 0.310798, indicating many false positives.
- F1-score: Balanced between precision and recall but still relatively low at 0.474212.
- Shows moderate performance with strong recall but poor precision.
- This indicates that while it captures all loan acceptors, it also incorrectly predicts many non-loan acceptors as positives.
- Useful for understanding the impact of hyperparameter tuning but not ideal for production.
3. Post-Pruned Model:
- Accuracy: Very high at 0.999714, close to the baseline model.
- Recall: Perfect at 100%, ensuring no loan acceptors are missed.
- Precision: High at 0.996988, minimizing false positives.
- F1-score: Near-perfect at 0.998492, balancing precision and recall effectively.
- Offers a good balance between complexity and performance.
- Maintains perfect recall while significantly improving precision compared to the pre-pruned model.
- Likely to generalize well to unseen data, making it the best candidate for deployment.
5. Business Implications
- Recall-focused Approach: All models achieve perfect recall, ensuring no potential loan customer is overlooked — critical for maximizing revenue opportunities.
- Precision Trade-off: The post-pruned model achieves high precision (0.996988), reducing unnecessary marketing efforts compared to the pre-pruned model.
- Final Recommendation: The post-pruned model is the most suitable choice because it balances recall and precision effectively, ensuring minimal wasted resources while capturing all potential loan customers.

"""

# Combine performance metrics of all three models into a single DataFrame for comparison on test data
models_test_comp_df = pd.concat(
    [
        decision_tree_perf_test.T,       # Baseline Decision Tree (default parameters)
        decision_tree_tune_perf_test.T,  # Pre-pruned Decision Tree (hyperparameter tuned)
        decision_tree_tune_post_test.T   # Post-pruned Decision Tree (cost-complexity pruned)
    ],
    axis=1  # Concatenate along columns
)

# Assign meaningful column names to identify each model
models_test_comp_df.columns = [
    "Decision Tree (sklearn default)",
    "Decision Tree (Pre-Pruning)",
    "Decision Tree (Post-Pruning)"
]

# Print a header for clarity
print("Test set performance comparison:")

# Display the final comparison DataFrame
models_test_comp_df

"""###Metrics for test set performance comparison
1. Baseline Decision Tree (sklearn default parameters)
- Performance: Achieves high accuracy (0.983333), recall (0.892617), precision (0.936620), and F1-score (0.914089).
- Interpretation: The baseline model performs well but may be overfitting, as it achieves near-perfect metrics on the training set but slightly lower scores on the test set.
- While achieving high accuracy and recall, this model may still be overfitted due to its complexity.
- Not ideal for deployment without further validation or simplification.
2. Pre-Pruned Decision Tree (hyperparameter tuned)
- Performance: Shows moderate performance with low accuracy (0.779333), perfect recall (1.0), low precision (0.310417), and low F1-score (0.473768).
- Interpretation: This model prioritizes capturing all loan acceptors (perfect recall), but at the cost of many false positives (low precision).
- It likely suffers from overfitting or aggressive pruning.
- Achieves perfect recall but at the expense of precision, indicating overly aggressive pruning.
- Suitable if the primary goal is to ensure no potential customer is missed, despite higher marketing costs.
3. Post-Pruned Decision Tree (cost-complexity pruned)
- Performance: Achieves strong performance with high accuracy (0.978000), good recall (0.845638), high precision (0.926471), and balanced F1-score (0.884211).
- Interpretation: This model strikes a good balance between recall and precision, ensuring minimal false negatives while avoiding excessive false positives.
- It generalizes well to unseen data.
- Offers a balanced approach with high accuracy, good recall, and high precision.
- Best suited for deployment as it generalizes well to unseen data while maintaining strong performance metrics.
- The post-pruned model is the most suitable choice because it balances recall and precision effectively, ensuring minimal wasted resources while capturing all potential loan customers.

##**Actionable Insights and Business Recommendations**

###Actionable Insights from EDA and Modeling
1. Income is the Most Important Predictor
- Customers earning more than $92.5K annually are significantly more likely to accept personal loans.
- In both baseline and tuned models, Income consistently ranked as the most important feature.
- Insight: Income determines purchasing power and eligibility for higher loan amounts.

2. Credit Card Spending (CCAvg) Strongly Influences Loan Acceptance
- High credit card users (spending > $2.95K/month) show increased likelihood of accepting a loan.
- This suggests they are comfortable with debt and have spending habits that align with borrowing behavior.
- Insight: CCAvg acts as a proxy for financial activity and readiness to take on additional credit.

3. Family Size Matters
- Smaller families (≤ 2 members) are more likely to accept loans.
- Larger families may be more cautious about taking on new debt due to higher living costs or responsibilities.
- Insight: Target individuals with smaller family sizes for better conversion rates.

4. Education Level Has a Moderate Influence
- Graduate and advanced degree holders are slightly more inclined to accept loans.
- This could be due to higher income or confidence in managing debt.
- Insight: Education level should be used as a secondary filter in segmentation strategies.

5. ZIP Code Variations Indicate Regional Opportunities
- Some ZIP codes (e.g., 92, 94, 93) showed higher acceptance rates.
- This could indicate regional economic strength or marketing effectiveness.
- Insight: Use geographic targeting for personalized campaigns in high-performing areas.

6. Online Banking Users Are More Likely to Accept Loans
- There's a slight but consistent increase in loan acceptance among online banking users.
- Insight: Digitally active customers are more engaged and possibly more responsive to digital campaigns.

7. CD Account Holders Show Higher Interest
- Customers with Certificate of Deposit (CD) accounts are more likely to accept personal loans.
- Insight: These customers already trust the bank with long-term deposits — building on this trust can help convert them into loan customers.

8. CreditCard Users Slightly Less Likely to Take Personal Loans
- Customers using credit cards from other banks were less likely to accept AllLife’s loan offers.
- Insight: May suggest brand loyalty or existing credit relationships elsewhere.

###Business Recommendations
1. Target High-Income Customers (Income > 92.5K)
- Focus marketing efforts on customers earning over 92.5K/year.
- Use personalized communication highlighting tailored loan options
- Offer pre-approved loan amounts based on income

2. Use Credit Card Usage as a Filter
- Prioritize customers who:
- Spend more than $2.95K/month on their credit cards
- Have high utilization but manageable balances
- Consider offering balance transfer or cash advance loans to these customers

3. Segment by Family Size
- Launch targeted campaigns toward customers with family size ≤ 2
- For larger families, consider joint loan products or co-borrower options

4. Leverage Education Level
- Run special campaigns for graduate-level and advanced degree holders
- Partner with alumni associations or professional networks
- Tailor messaging around career growth, home improvement, or investments

5. Geographic Targeting
- Identify top ZIP codes (e.g., 92, 94, 93) where loan conversion is highest
- Use local advertising or community events to boost awareness
- Analyze why certain regions perform better — adjust product offerings accordingly

6. Engage Online Banking Users
- Send push notifications, emails, or in-app messages to online banking users
- Simplify the application process through mobile/digital platforms
- Offer instant approval or quick decision tools

7. Cross-Sell to CD & Mortgage Customers
- CD account holders already trust the bank — offer bundled services
- Mortgage holders may be interested in refinancing or equity-based loans
- Create loyalty programs or exclusive offers for these groups

___
"""

